{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVSo1+PPvK/FbgS6ZW9iEo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BillyMuthiani/Ultra_fast-Book_summarizer/blob/main/Book_Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf==1.24.7 bert-extractive-summarizer==0.10.1 nltk sentence-transformers python-docx fpdf tqdm torch huggingface-hub transformers numpy safetensors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g04HdGKWmNx",
        "outputId": "3c930b6a-2cb6-4af8-9b60-184e1efd1d7a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf==1.24.7\n",
            "  Downloading PyMuPDF-1.24.7-cp311-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting bert-extractive-summarizer==0.10.1\n",
            "  Downloading bert_extractive_summarizer-0.10.1-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (0.33.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.3)\n",
            "Collecting PyMuPDFb==1.24.6 (from pymupdf==1.24.7)\n",
            "  Downloading PyMuPDFb-1.24.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from bert-extractive-summarizer==0.10.1) (1.6.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (from bert-extractive-summarizer==0.10.1) (3.8.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.7.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (1.1.5)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2025.7.14)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->bert-extractive-summarizer==0.10.1) (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer==0.10.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer==0.10.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer==0.10.1) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer==0.10.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer==0.10.1) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer==0.10.1) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer==0.10.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer==0.10.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer==0.10.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer==0.10.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer==0.10.1) (0.16.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer==0.10.1) (2.11.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer==0.10.1) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer==0.10.1) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->bert-extractive-summarizer==0.10.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->bert-extractive-summarizer==0.10.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->bert-extractive-summarizer==0.10.1) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->bert-extractive-summarizer==0.10.1) (0.4.1)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->bert-extractive-summarizer==0.10.1) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->bert-extractive-summarizer==0.10.1) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer==0.10.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer==0.10.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->bert-extractive-summarizer==0.10.1) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->bert-extractive-summarizer==0.10.1) (7.3.0.post1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->bert-extractive-summarizer==0.10.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer==0.10.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer==0.10.1) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->bert-extractive-summarizer==0.10.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer==0.10.1) (0.1.2)\n",
            "Downloading PyMuPDF-1.24.7-cp311-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bert_extractive_summarizer-0.10.1-py3-none-any.whl (25 kB)\n",
            "Downloading PyMuPDFb-1.24.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=85166d5b7b99a5110e6a76af35f70c5af2e478b445bd6f90a4940fbbc647b795\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/66/bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf, python-docx, PyMuPDFb, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, pymupdf, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert-extractive-summarizer\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed PyMuPDFb-1.24.6 bert-extractive-summarizer-0.10.1 fpdf-1.7.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pymupdf-1.24.7 python-docx-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF for PDF processing\n",
        "import nltk\n",
        "import speech_recognition as sr\n",
        "import requests\n",
        "from transformers import pipeline\n",
        "import multiprocessing as mp\n",
        "import docx\n",
        "from fpdf import FPDF\n",
        "import uuid\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import unicodedata\n",
        "import torch\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Set multiprocessing start method for CUDA compatibility\n",
        "if torch.cuda.is_available():\n",
        "    mp.set_start_method('spawn', force=True)\n",
        "\n",
        "# Cache summarization pipeline globally\n",
        "MODEL_NAME = 'sshleifer/distilbart-cnn-6-6'\n",
        "\n",
        "def load_summarizer():\n",
        "    try:\n",
        "        logging.info(f\"Loading summarization pipeline: {MODEL_NAME}\")\n",
        "        # Use GPU if available (device=0), else CPU (device=-1)\n",
        "        device = 0 if torch.cuda.is_available() else -1\n",
        "        summarizer = pipeline(\"summarization\", model=MODEL_NAME, tokenizer=MODEL_NAME, device=device)\n",
        "        return summarizer\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to load {MODEL_NAME}: {e}\")\n",
        "        return None\n",
        "\n",
        "SUMMARIZER = load_summarizer()\n",
        "\n",
        "# Function to preprocess text for PDF compatibility\n",
        "def preprocess_text_for_pdf(text):\n",
        "    try:\n",
        "        # Normalize Unicode characters to ASCII\n",
        "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error preprocessing text: {e}\")\n",
        "        return text\n",
        "\n",
        "# Function to truncate text to token limit\n",
        "def truncate_to_token_limit(text, max_tokens=512):\n",
        "    try:\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        truncated_text = \"\"\n",
        "        token_count = 0\n",
        "        for sentence in sentences:\n",
        "            # Estimate tokens (rough: 1 word ~ 1.3 tokens)\n",
        "            sentence_tokens = len(sentence.split()) * 1.3\n",
        "            if token_count + sentence_tokens <= max_tokens:\n",
        "                truncated_text += sentence + \" \"\n",
        "                token_count += sentence_tokens\n",
        "            else:\n",
        "                break\n",
        "        return truncated_text.strip()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error truncating text: {e}\")\n",
        "        return text[:max_tokens]\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        text = \"\"\n",
        "        # Sample every 10th page for more content\n",
        "        for page_num in range(0, len(doc), 10):\n",
        "            text += doc[page_num].get_text()\n",
        "        doc.close()\n",
        "        logging.info(f\"PDF extraction time: {time.time() - start_time:.2f} seconds\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error reading PDF: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to extract text from DOCX\n",
        "def extract_text_from_docx(docx_path):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        doc = docx.Document(docx_path)\n",
        "        text = \"\"\n",
        "        total_paras = len(doc.paragraphs)\n",
        "        # Extract first 10% of paragraphs\n",
        "        for i, para in enumerate(doc.paragraphs):\n",
        "            if i < total_paras * 0.10:\n",
        "                text += para.text + \"\\n\"\n",
        "        logging.info(f\"DOCX extraction time: {time.time() - start_time:.2f} seconds\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error reading DOCX: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function for speech-to-text conversion\n",
        "def speech_to_text():\n",
        "    start_time = time.time()\n",
        "    recognizer = sr.Recognizer()\n",
        "    with sr.Microphone() as source:\n",
        "        print(\"Listening for input...\")\n",
        "        recognizer.adjust_for_ambient_noise(source)\n",
        "        audio = recognizer.listen(source)\n",
        "        try:\n",
        "            text = recognizer.recognize_google(audio)\n",
        "            logging.info(f\"Speech-to-text time: {time.time() - start_time:.2f} seconds\")\n",
        "            print(\"Transcribed text:\", text)\n",
        "            return text\n",
        "        except sr.UnknownValueError:\n",
        "            logging.error(\"Could not understand audio\")\n",
        "            return None\n",
        "        except sr.RequestError as e:\n",
        "            logging.error(f\"Speech recognition error: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "# Function to download a file with progress bar\n",
        "def download_file(url, local_filename):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        with requests.get(url, stream=True) as r:\n",
        "            r.raise_for_status()\n",
        "            total_size = int(r.headers.get('content-length', 0))\n",
        "            with open(local_filename, 'wb') as f, tqdm(\n",
        "                desc=f\"Downloading {local_filename}\",\n",
        "                total=total_size,\n",
        "                unit='iB',\n",
        "                unit_scale=True,\n",
        "                unit_divisor=1024,\n",
        "            ) as bar:\n",
        "                for data in r.iter_content(chunk_size=1024):\n",
        "                    size = f.write(data)\n",
        "                    bar.update(size)\n",
        "        logging.info(f\"Download time: {time.time() - start_time:.2f} seconds\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error downloading file: {e}\")\n",
        "        raise\n",
        "\n",
        "# Function to summarize a single chunk\n",
        "def summarize_chunk(chunk):\n",
        "    try:\n",
        "        if SUMMARIZER is None:\n",
        "            logging.error(\"Summarizer pipeline not loaded\")\n",
        "            return \"\"\n",
        "        # Ensure chunk is not empty and within model limits\n",
        "        if not chunk.strip():\n",
        "            logging.warning(\"Empty chunk received\")\n",
        "            return \"\"\n",
        "        # Truncate to max length (DistilBART supports ~1024 tokens)\n",
        "        max_input_length = 500\n",
        "        chunk = chunk[:max_input_length]\n",
        "        summary = SUMMARIZER(chunk, max_length=150, min_length=30, do_sample=False)[0]['summary_text']\n",
        "        logging.info(f\"Successfully summarized chunk of length {len(chunk)}\")\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error summarizing chunk: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Function to summarize text using parallel processing with progress bar\n",
        "def summarize_text(text, max_pages=5):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        # Split text into chunks (500 characters each for speed)\n",
        "        chunk_size = 500\n",
        "        text_chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "        # Limit to 15 chunks to balance content and speed\n",
        "        text_chunks = text_chunks[:15]\n",
        "        logging.info(f\"Split text into {len(text_chunks)} chunks\")\n",
        "\n",
        "        # Parallel processing with multiprocessing and progress bar\n",
        "        with mp.Pool(processes=4) as pool:\n",
        "            summaries = list(tqdm(pool.imap(summarize_chunk, text_chunks), total=len(text_chunks), desc=\"Summarizing chunks\"))\n",
        "\n",
        "        # Combine summaries and refine\n",
        "        combined_summary = \" \".join([s for s in summaries if s])  # Filter out empty summaries\n",
        "        if not combined_summary:\n",
        "            logging.warning(\"No valid summaries generated\")\n",
        "            return None\n",
        "        # Truncate combined summary to 512 tokens for final summarization\n",
        "        combined_summary = truncate_to_token_limit(combined_summary, max_tokens=512)\n",
        "        if SUMMARIZER is None:\n",
        "            logging.error(\"Summarizer pipeline not loaded\")\n",
        "            return None\n",
        "        final_summary = SUMMARIZER(combined_summary, max_length=300, min_length=50, do_sample=False)[0]['summary_text']\n",
        "        logging.info(f\"Summarization time: {time.time() - start_time:.2f} seconds\")\n",
        "        return final_summary\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error summarizing text: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to generate PDF summary\n",
        "def generate_pdf_summary(summary, output_path, title=\"Summary\"):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        pdf = FPDF()\n",
        "        pdf.set_auto_page_break(auto=True, margin=15)\n",
        "        pdf.add_page()\n",
        "        pdf.set_font(\"Arial\", size=12)\n",
        "\n",
        "        # Add title\n",
        "        pdf.set_font(\"Arial\", \"B\", 16)\n",
        "        title = preprocess_text_for_pdf(title)  # Preprocess title\n",
        "        pdf.cell(0, 10, title, 0, 1, \"C\")\n",
        "        pdf.ln(10)\n",
        "\n",
        "        # Add summary text\n",
        "        pdf.set_font(\"Arial\", size=12)\n",
        "        summary = preprocess_text_for_pdf(summary)  # Preprocess summary\n",
        "        pdf.multi_cell(0, 10, summary)\n",
        "\n",
        "        # Save PDF\n",
        "        pdf.output(output_path)\n",
        "        logging.info(f\"PDF generation time: {time.time() - start_time:.2f} seconds\")\n",
        "        logging.info(f\"PDF summary saved to {output_path}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating PDF: {e}\")\n",
        "\n",
        "# Main function to run the app\n",
        "def main():\n",
        "    total_start_time = time.time()\n",
        "    print(\"Welcome to the Ultra-Fast Book Summarization App!\")\n",
        "    print(\"Summarize books in seconds, including locally downloaded PDFs.\")\n",
        "    print(\"1. Summarize from PDF\")\n",
        "    print(\"2. Summarize from DOCX\")\n",
        "\n",
        "\n",
        "    choice = input(\"Select an option (1-2): \")\n",
        "    text = None\n",
        "    title = \"Summary\"\n",
        "    local_path = None\n",
        "\n",
        "    if choice == \"1\":\n",
        "        file_path = input(\"Enter PDF file path: \")\n",
        "        if os.path.exists(file_path):\n",
        "            text = extract_text_from_pdf(file_path)\n",
        "            title = os.path.basename(file_path)\n",
        "            local_path = file_path\n",
        "        else:\n",
        "            logging.error(\"File not found\")\n",
        "            print(\"File not found\")\n",
        "            return\n",
        "    elif choice == \"2\":\n",
        "        file_path = input(\"Enter DOCX file path: \")\n",
        "        if os.path.exists(file_path):\n",
        "            text = extract_text_from_docx(file_path)\n",
        "            title = os.path.basename(file_path)\n",
        "            local_path = file_path\n",
        "        else:\n",
        "            logging.error(\"File not found\")\n",
        "            print(\"File not found\")\n",
        "            return\n",
        "    elif choice == \"3\":\n",
        "        use_speech = input(\"Use speech-to-text? (y/n): \").lower() == 'y'\n",
        "        if use_speech:\n",
        "            text = speech_to_text()\n",
        "            title = \"Speech Summary\"\n",
        "\n",
        "    else:\n",
        "        logging.error(\"Invalid choice\")\n",
        "        print(\"Invalid choice\")\n",
        "        return\n",
        "\n",
        "    if text:\n",
        "        summary = summarize_text(text)\n",
        "        if summary:\n",
        "            output_path = f\"summary_{uuid.uuid4()}.pdf\"\n",
        "            generate_pdf_summary(summary, output_path, title)\n",
        "        else:\n",
        "            logging.error(\"Summarization failed\")\n",
        "            print(\"Summarization failed\")\n",
        "    else:\n",
        "        logging.error(\"No text to summarize\")\n",
        "        print(\"No text to summarize\")\n",
        "\n",
        "    logging.info(f\"Total execution time: {time.time() - total_start_time:.2f} seconds\")\n",
        "    print(f\"Total execution time: {time.time() - total_start_time:.2f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7erkDvNpEoG",
        "outputId": "62e91af1-7beb-4286-e171-fe054325e87a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the Ultra-Fast Book Summarization App!\n",
            "Summarize books in seconds, including locally downloaded PDFs.\n",
            "1. Summarize from PDF\n",
            "2. Summarize from DOCX\n",
            "Select an option (1-2): 1\n",
            "Enter PDF file path: /content/The Alchemist (Paulo Coelho) (Z-Library).pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSummarizing chunks:   0%|          | 0/15 [00:00<?, ?it/s]Your max_length is set to 150, but your input_length is only 137. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=68)\n",
            "Your max_length is set to 150, but your input_length is only 135. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=67)\n",
            "Your max_length is set to 150, but your input_length is only 141. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=70)\n",
            "Your max_length is set to 150, but your input_length is only 116. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=58)\n",
            "Your max_length is set to 150, but your input_length is only 141. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=70)\n",
            "Your max_length is set to 150, but your input_length is only 144. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=72)\n",
            "Your max_length is set to 150, but your input_length is only 141. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=70)\n",
            "Summarizing chunks:   7%|▋         | 1/15 [00:45<10:36, 45.47s/it]Your max_length is set to 150, but your input_length is only 137. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=68)\n",
            "Your max_length is set to 150, but your input_length is only 119. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=59)\n",
            "Your max_length is set to 150, but your input_length is only 119. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=59)\n",
            "Your max_length is set to 150, but your input_length is only 141. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=70)\n",
            "Summarizing chunks:  33%|███▎      | 5/15 [01:28<02:39, 15.91s/it]Your max_length is set to 150, but your input_length is only 146. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=73)\n",
            "Summarizing chunks:  60%|██████    | 9/15 [01:54<01:04, 10.82s/it]Your max_length is set to 150, but your input_length is only 136. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=68)\n",
            "Your max_length is set to 150, but your input_length is only 130. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=65)\n",
            "Your max_length is set to 150, but your input_length is only 124. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=62)\n",
            "Summarizing chunks: 100%|██████████| 15/15 [02:35<00:00, 10.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total execution time: 192.88 seconds\n"
          ]
        }
      ]
    }
  ]
}